{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim是一个免费的 Python库，旨在自动从文档中提取语义主题，尽可能有效（计算机方式）和不费力（人性化）。  \n",
    "Gensim旨在处理原始的非结构化数字文本（“ 纯文本 ”）。gensim中的算法如潜在语义分析，潜在狄利克雷分配和随机预测 通过检查训练文档语料库中单词的统计共现模式来发现文档的语义结构。这些算法是无监督的，这意味着不需要人工输入 - 你只需要一个纯文本文档的语料库。\n",
    "一旦找到这些统计模式，任何纯文本文档都可以用新的语义表示简洁地表达，并且针对与其他文档的主题相似性进行查询。**  \n",
    "\n",
    "**核心概念**  \n",
    "整个gensim软件包围绕着语料库，矢量和 模型的概念。\n",
    "\n",
    "**文集**  \n",
    "数字文件的集合。该集合用于自动推断文档结构，主题等。因此，该集合也称为训练语料库。这个推断的潜在结构可以在以后用于将主题分配给新文档，这些文档没有出现在训练语料库中。不需要人工干预（如手动标记文档或创建其他元数据）。  \n",
    "**向量**  \n",
    "在矢量空间模型（VSM）中，每个文档都由一组特征表示。例如，一个特征可以被认为是一个问题 - 答案对：\n",
    "\n",
    "单词splonge在文档中出现多少次？零。\n",
    "文件包含多少段？二。\n",
    "文档使用多少种字体？五。\n",
    "问题通常只能由它的一个整数标识符（如所表示1，2和3在此），因此，该文件的表示变得一系列像对。如果我们事先知道所有问题，我们可以让它们隐含并且简单地写。这个答案序列可以被认为是一个向量（在这种情况下是一个三维向量）。出于实际的目的，只允许回答（或可以转换为）单个实数的问题。(1, 0.0), (2, 2.0), (3, 5.0)(0.0, 2.0, 5.0)\n",
    "\n",
    "每个文件的问题都是一样的，所以通过查看两个向量（代表两个文件），我们希望能够得出如下结论：“这两个向量中的数字非常相似，因此原始文件必须相似也是“。当然，这样的结论是否符合现实取决于我们如何选择我们的问题。\n",
    "\n",
    "**稀疏矢量**  \n",
    "通常情况下，大多数问题的答案是0.0。为了节省空间，我们从文档的表示中省略它们，并且只写（注意缺失的）。由于所有问题的集合都是事先知道的，所以文档的稀疏表示中的所有缺失特征可以明确地解析为零。(2, 2.0), (3, 5.0)(1, 0.0)0.0\n",
    "\n",
    "Gensim没有规定任何特定的语料库格式; 一个语料库就是当迭代时连续产生这些稀疏向量的东西。例如，set（（（（2，2.0），（3，5.0）），（（0,1.0），（3,1.0））））是两个文档的平凡语料库，每个文档都有两个非零特征-对组合。\n",
    "\n",
    "**模型**  \n",
    "我们使用模型作为抽象术语，指的是从一个文档表示到另一个文档表示的转换。在gensim中，文档被表示为向量，所以模型可以被认为是两个向量空间之间的转换。这个转变的细节是从训练语料库中学到的。\n",
    "\n",
    "例如，考虑一种转换，它采用单词出现的原始计数并对它们进行加权，以便打折常用单词并提升罕见单词。任何特定单词加权的确切数量由训练语料库中该单词的相对频率决定。当我们应用这个模型时，我们从一个向量空间（包含原始字数）转换为另一个（包含加权计数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01语料库和矢量空间(Corpora_and_Vector _Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)#记录日志事件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从字符串到向量（From Strings to Vectors）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2018-03-21 10:30:12,002 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "#几个不同的句子或者文档\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "              \"A survey of user opinion of computer system response time\",\n",
    "              \"The EPS user interface management system\",\n",
    "              \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'], ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'management', 'system'], ['system', 'human', 'system', 'engineering', 'testing', 'eps'], ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', 'binary', 'unordered', 'trees'], ['intersection', 'graph', 'paths', 'trees'], ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "\n",
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "#进行简单的文本处理(停用词)\n",
    "stoplist = set('for a of the and to in'.split())#停用词\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] for document in documents]\n",
    "print(texts);print()\n",
    "\n",
    "#删除只出现一次的单词\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token  for token in text if frequency[token] > 1] for text in texts]\n",
    "print(texts);print()\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:12,177 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-21 10:30:12,181 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2018-03-21 10:30:12,184 : INFO : saving Dictionary object under deerwester.dict, separately None\n",
      "2018-03-21 10:30:12,192 : INFO : saved deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n",
      "\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n",
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "#问题和ID之间的映射称为字典\n",
    "dictionary = corpora.Dictionary(documents=texts)\n",
    "dictionary.save('deerwester.dict') # store the dictionary, for future reference\n",
    "print(dictionary);print()\n",
    "\n",
    "#我们看到处理过的语料库中有十二个不同的单词，这意味着每个文档将由十二个数字表示（即，由一个12-D矢量）\n",
    "print(dictionary.token2id)\n",
    "\n",
    "#将标记化文档转换为矢量\n",
    "new_doc = \"Human computer interaction\"\n",
    "#该函数doc2bow()简单地计算每个不同单词的出现次数，将该单词转换为其整数单词id并将结果作为稀疏向量返回。\n",
    "new_vec = dictionary.doc2bow(document=new_doc.lower().split())\n",
    "print(new_vec)  # 单词 \"interaction\"没有出现过就被忽略了 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:12,303 : INFO : storing corpus in Matrix Market format to deerwester.mm\n",
      "2018-03-21 10:30:12,308 : INFO : saving sparse matrix to deerwester.mm\n",
      "2018-03-21 10:30:12,311 : INFO : PROGRESS: saving document #0\n",
      "2018-03-21 10:30:12,316 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2018-03-21 10:30:12,319 : INFO : saving MmCorpus index to deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#处理好的语料生成\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('deerwester.mm', corpus) # store to disk, for later use\n",
    "pprint(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料库流 - 一次一个文件(Corpus Streaming – One Document at a Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Mycorpus object at 0x0000000011596470>\n",
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "#上面的小文本停留在内存(RAM)中，没问题，当文档很大时，就需要留处理了(前提是问题和ID之间的映射称为字典已经生成)\n",
    "class Mycorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt'):\n",
    "            yield dictionary.doc2bow(line.lower().split())\n",
    "            \n",
    "corpus_memory_friendly = Mycorpus()\n",
    "print(corpus_memory_friendly)\n",
    "\n",
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:12,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-21 10:30:12,535 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...)\n",
      "\n",
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "#构建字典而不将所有文本加载到内存中\n",
    "from six import iteritems\n",
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "print(dictionary);print()\n",
    "\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "dictionary.filter_tokens(stop_ids + once_ids)# remove stop words and words that appear only once\n",
    "dictionary.compactify()# remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料库格式(Corpus Formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存在用于将矢量空间语料库（〜矢量序列）序列化到磁盘的多种文件格式。 Gensim通过前面提到的流式语料库接口实现它们：文档以懒惰的方式从一个文件读取（或存储到），一次一个文档，而不会将整个语料库一次读入主存储器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:12,739 : INFO : storing corpus in Matrix Market format to corpus.mm\n",
      "2018-03-21 10:30:12,742 : INFO : saving sparse matrix to corpus.mm\n",
      "2018-03-21 10:30:12,743 : INFO : PROGRESS: saving document #0\n",
      "2018-03-21 10:30:12,746 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2018-03-21 10:30:12,749 : INFO : saving MmCorpus index to corpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "# create a toy corpus of 2 documents, as a plain Python list\n",
    "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
    "corpora.MmCorpus.serialize('corpus.mm', corpus) #市场矩阵格式(Market Matrix format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:12,872 : INFO : converting corpus to SVMlight format: corpus.svmlight\n",
      "2018-03-21 10:30:12,874 : INFO : saving SvmLightCorpus index to corpus.svmlight.index\n",
      "2018-03-21 10:30:12,877 : INFO : no word id mapping provided; initializing from corpus\n",
      "2018-03-21 10:30:12,878 : INFO : storing corpus in Blei's LDA-C format into corpus.lda-c\n",
      "2018-03-21 10:30:12,881 : INFO : saving vocabulary of 2 words to corpus.lda-c.vocab\n",
      "2018-03-21 10:30:12,883 : INFO : saving BleiCorpus index to corpus.lda-c.index\n",
      "2018-03-21 10:30:12,887 : INFO : no word id mapping provided; initializing from corpus\n",
      "2018-03-21 10:30:12,889 : INFO : storing corpus in List-Of-Words format into corpus.low\n",
      "2018-03-21 10:30:12,895 : WARNING : List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value\n",
      "2018-03-21 10:30:12,898 : INFO : saving LowCorpus index to corpus.low.index\n"
     ]
    }
   ],
   "source": [
    "#其他格式包括Joachim的SVMlight格式， Blei的LDA-C格式和 GibbsLDA ++格式\n",
    "# Joachim’s SVMlight format, Blei’s LDA-C format and GibbsLDA++ format.\n",
    "corpora.SvmLightCorpus.serialize('corpus.svmlight', corpus)\n",
    "corpora.BleiCorpus.serialize('corpus.lda-c', corpus)\n",
    "corpora.LowCorpus.serialize('corpus.low', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:30:13,176 : INFO : loaded corpus index from corpus.mm.index\n",
      "2018-03-21 10:30:13,179 : INFO : initializing cython corpus reader from corpus.mm\n",
      "2018-03-21 10:30:13,183 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n",
      "2018-03-21 10:30:13,188 : INFO : no word id mapping provided; initializing from corpus\n",
      "2018-03-21 10:30:13,190 : INFO : storing corpus in Blei's LDA-C format into corpus.lda-c\n",
      "2018-03-21 10:30:13,194 : INFO : saving vocabulary of 2 words to corpus.lda-c.vocab\n",
      "2018-03-21 10:30:13,198 : INFO : saving BleiCorpus index to corpus.lda-c.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n",
      "[[(1, 0.5)], []]\n",
      "[(1, 0.5)]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#相反，要从Matrix Market文件加载语料库迭代器：\n",
    "corpus = corpora.MmCorpus('corpus.mm')\n",
    "#语料库对象是流，因此通常您将无法直接打印它们：\n",
    "print(corpus)\n",
    "#相反，要查看语料库的内容：\n",
    "print(list(corpus))\n",
    "for doc in corpus:\n",
    "     print(doc)\n",
    "#再转换成其他格式\n",
    "corpora.BleiCorpus.serialize('corpus.lda-c', corpus)\n",
    "\n",
    "#通过这种方式，gensim也可以用作高效的I / O格式转换工具：只需使用一种格式加载文档流，并立即以另一种格式保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 与NumPy和SciPy兼容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'number_of_corpus_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-49d76fbd9544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumpy_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# random matrix as an example\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnumpy_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus2dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_terms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_corpus_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'number_of_corpus_features' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "numpy_matrix = np.random.randint(10, size=[5,2])  # random matrix as an example\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "scipy_sparse_matrix = scipy.sparse.random(5,2)  # random sparse matrix as example\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02主题和转换(Topics and Transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:42:51,173 : INFO : loading Dictionary object from deerwester.dict\n",
      "2018-03-21 10:42:51,176 : INFO : loaded deerwester.dict\n",
      "2018-03-21 10:42:51,179 : INFO : loaded corpus index from deerwester.mm.index\n",
      "2018-03-21 10:42:51,181 : INFO : initializing cython corpus reader from deerwester.mm\n",
      "2018-03-21 10:42:51,185 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used files generated from first tutorial\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "if(os.path.exists('deerwester.dict')):\n",
    "    dictionary = corpora.Dictionary.load('deerwester.dict')\n",
    "    corpus = corpora.MmCorpus('deerwester.mm')\n",
    "    print(\"Used files generated from first tutorial\")\n",
    "else:\n",
    "    print(\"Please run first tutorial to generate data set\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在本教程中，我将演示如何将文档从一个矢量表示转换为另一个矢量表示。这个过程有两个目标：\n",
    "\n",
    "为了带出语料库中隐藏的结构，发现词之间的关系，并用它们以新的（有希望的）更有意义的方式描述文档。\n",
    "\n",
    "使文档表示更加紧凑。这既提高了效率（新的表示消耗更少的资源）和有效性（边际数据趋势被忽略，降低噪音）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 创建转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**转换总是在两个特定的向量空间之间转换。必须使用相同的向量空间（=相同的特征ID集合）进行训练以及后续的向量变换。如果不能使用相同的输入特征空间，例如应用不同的字符串预处理，使用不同的特征标识，或者在预期TfIdf矢量的情况下使用袋字输入向量，将导致转换调用期间的功能不匹配，从而导致垃圾输出和/或运行时异常。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:46:13,713 : INFO : collecting document frequencies\n",
      "2018-03-21 10:46:13,715 : INFO : PROGRESS: processing document #0\n",
      "2018-03-21 10:46:13,717 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "#转换是标准的Python对象，通常通过训练语料库进行初始化：\n",
    "tfidf = models.TfidfModel(corpus=corpus)# step 1 -- initialize a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换矢量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n",
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "#从现在开始，tfidf被视为只读对象，可用于将任何向量从旧的表示（bag-of-words整数计数）转换为新的表示（TfIdf实值权重）：\n",
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow]) # step 2 -- use the model to transform vectors\n",
    "\n",
    "#或者对整个语料库应用转换：\n",
    "#(调用model[corpus]仅在旧corpus 文档流中创建一个包装- 实际转换在文档迭代期间即时完成。)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 10:56:30,673 : INFO : using serial LSI version on this node\n",
      "2018-03-21 10:56:30,674 : INFO : updating model with new documents\n",
      "2018-03-21 10:56:30,676 : INFO : preparing a new chunk of documents\n",
      "2018-03-21 10:56:30,677 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-03-21 10:56:30,678 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2018-03-21 10:56:30,680 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2018-03-21 10:56:30,682 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-03-21 10:56:30,683 : INFO : computing the final decomposition\n",
      "2018-03-21 10:56:30,685 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2018-03-21 10:56:30,686 : INFO : processed documents up to #9\n",
      "2018-03-21 10:56:30,687 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-03-21 10:56:30,689 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n",
      "2018-03-21 10:56:30,691 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-03-21 10:56:30,693 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2) # initialize an LSI transformation\n",
    "corpus_lsi = lsi[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 11:03:48,405 : INFO : saving Projection object under model.lsi.projection, separately None\n",
      "2018-03-21 11:03:48,409 : INFO : saved model.lsi.projection\n",
      "2018-03-21 11:03:48,410 : INFO : saving LsiModel object under model.lsi, separately None\n",
      "2018-03-21 11:03:48,412 : INFO : not storing attribute projection\n",
      "2018-03-21 11:03:48,413 : INFO : not storing attribute dispatcher\n",
      "2018-03-21 11:03:48,416 : INFO : saved model.lsi\n",
      "2018-03-21 11:03:48,418 : INFO : loading LsiModel object from model.lsi\n",
      "2018-03-21 11:03:48,421 : INFO : loading id2word recursively from model.lsi.id2word.* with mmap=None\n",
      "2018-03-21 11:03:48,424 : INFO : setting ignored attribute projection to None\n",
      "2018-03-21 11:03:48,426 : INFO : setting ignored attribute dispatcher to None\n",
      "2018-03-21 11:03:48,428 : INFO : loaded model.lsi\n",
      "2018-03-21 11:03:48,431 : INFO : loading LsiModel object from model.lsi.projection\n",
      "2018-03-21 11:03:48,435 : INFO : loaded model.lsi.projection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090572), (1, -0.5200703306361847)]\n",
      "[(0, 0.19667592859142874), (1, -0.7609563167700035)]\n",
      "[(0, 0.08992639972446731), (1, -0.7241860626752509)]\n",
      "[(0, 0.07585847652178428), (1, -0.632055158600343)]\n",
      "[(0, 0.10150299184980405), (1, -0.5737308483002947)]\n",
      "[(0, 0.7032108939378298), (1, 0.1611518021402609)]\n",
      "[(0, 0.8774787673119819), (1, 0.1675890686465978)]\n",
      "[(0, 0.9098624686818566), (1, 0.140865536287194)]\n",
      "[(0, 0.6165825350569283), (1, -0.053929075663891005)]\n"
     ]
    }
   ],
   "source": [
    "#根据LSI的说法，“树”，“图”和“选修”都是相关词（对第一个主题的方向贡献最大），而第二个主题实际上与所有其他词有关。正如预期的那样，前五个文件与第二个主题更密切相关，而剩下的四个文件涉及第一个主题：\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)\n",
    "\n",
    "[(0, -0.066), (1, 0.520)] # \"Human machine interface for lab abc computer applications\"\n",
    "[(0, -0.197), (1, 0.761)] # \"A survey of user opinion of computer system response time\"\n",
    "[(0, -0.090), (1, 0.724)] # \"The EPS user interface management system\"\n",
    "[(0, -0.076), (1, 0.632)] # \"System and human system engineering testing of EPS\"\n",
    "[(0, -0.102), (1, 0.574)] # \"Relation of user perceived response time to error measurement\"\n",
    "[(0, -0.703), (1, -0.161)] # \"The generation of random binary unordered trees\"\n",
    "[(0, -0.877), (1, -0.168)] # \"The intersection graph of paths in trees\"\n",
    "[(0, -0.910), (1, -0.141)] # \"Graph minors IV Widths of trees and well quasi ordering\"\n",
    "[(0, -0.617), (1, 0.054)] # \"Graph minors A survey\"\n",
    "\n",
    "\n",
    "#通过save和load进行持久化操作\n",
    "lsi.save('model.lsi')# same for tfidf, lda, ...\n",
    "lsi = models.LsiModel.load('model.lsi')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可用的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-21 23:19:44,421 : INFO : collecting document frequencies\n",
      "2018-03-21 23:19:44,434 : INFO : PROGRESS: processing document #0\n",
      "2018-03-21 23:19:44,435 : INFO : calculating IDF weights for 9 documents and 11 features (28 matrix non-zeros)\n",
      "2018-03-21 23:19:44,436 : INFO : using serial LSI version on this node\n",
      "2018-03-21 23:19:44,437 : INFO : updating model with new documents\n",
      "2018-03-21 23:19:44,439 : INFO : preparing a new chunk of documents\n",
      "2018-03-21 23:19:44,440 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-03-21 23:19:44,441 : INFO : 1st phase: constructing (12, 400) action matrix\n",
      "2018-03-21 23:19:44,444 : INFO : orthonormalizing (12, 400) action matrix\n",
      "2018-03-21 23:19:44,446 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-03-21 23:19:44,448 : INFO : computing the final decomposition\n",
      "2018-03-21 23:19:44,453 : INFO : keeping 9 factors (discarding 0.000% of energy spectrum)\n",
      "2018-03-21 23:19:44,456 : INFO : processed documents up to #9\n",
      "2018-03-21 23:19:44,458 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"response\" + 0.060*\"time\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2018-03-21 23:19:44,459 : INFO : topic #1(1.476): 0.460*\"system\" + 0.373*\"user\" + 0.332*\"eps\" + 0.328*\"interface\" + 0.320*\"time\" + 0.320*\"response\" + 0.293*\"computer\" + 0.280*\"human\" + 0.171*\"survey\" + -0.161*\"trees\"\n",
      "2018-03-21 23:19:44,461 : INFO : topic #2(1.191): 0.456*\"time\" + 0.456*\"response\" + -0.352*\"eps\" + -0.340*\"human\" + -0.318*\"interface\" + -0.277*\"system\" + 0.272*\"survey\" + 0.213*\"user\" + -0.183*\"trees\" + 0.114*\"minors\"\n",
      "2018-03-21 23:19:44,463 : INFO : topic #3(1.043): -0.583*\"trees\" + 0.556*\"minors\" + 0.399*\"survey\" + 0.256*\"graph\" + -0.211*\"time\" + -0.211*\"response\" + -0.160*\"user\" + 0.081*\"human\" + 0.038*\"interface\" + 0.035*\"system\"\n",
      "2018-03-21 23:19:44,465 : INFO : topic #4(0.884): -0.611*\"computer\" + 0.425*\"system\" + 0.420*\"eps\" + -0.354*\"interface\" + -0.339*\"human\" + 0.148*\"user\" + 0.058*\"minors\" + -0.047*\"trees\" + 0.034*\"graph\" + -0.027*\"survey\"\n",
      "2018-03-21 23:19:44,495 : INFO : no word id mapping provided; initializing from corpus, assuming identity\n",
      "2018-03-21 23:19:44,541 : INFO : constructing (500, 12) random matrix\n",
      "2018-03-21 23:19:44,730 : INFO : using symmetric alpha at 0.01\n",
      "2018-03-21 23:19:44,732 : INFO : using symmetric eta at 0.01\n",
      "2018-03-21 23:19:44,764 : INFO : using serial LDA version on this node\n",
      "2018-03-21 23:19:44,870 : INFO : running online (single-pass) LDA training, 100 topics, 1 passes over the supplied corpus of 9 documents, updating model once every 9 documents, evaluating perplexity every 9 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-03-21 23:19:44,872 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-03-21 23:19:45,430 : INFO : -124.578 per-word bound, 31758014243925841304533155208698527744.0 perplexity estimate based on a held-out corpus of 9 documents with 29 words\n",
      "2018-03-21 23:19:45,433 : INFO : PROGRESS: pass 0, at document #9/9\n",
      "2018-03-21 23:19:45,510 : INFO : topic #59 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-03-21 23:19:45,512 : INFO : topic #25 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-03-21 23:19:45,514 : INFO : topic #83 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-03-21 23:19:45,515 : INFO : topic #84 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-03-21 23:19:45,517 : INFO : topic #37 (0.010): 0.083*\"user\" + 0.083*\"system\" + 0.083*\"graph\" + 0.083*\"trees\" + 0.083*\"eps\" + 0.083*\"computer\" + 0.083*\"time\" + 0.083*\"interface\" + 0.083*\"response\" + 0.083*\"human\"\n",
      "2018-03-21 23:19:45,525 : INFO : topic diff=87.389175, rho=1.000000\n",
      "2018-03-21 23:19:45,634 : INFO : (0, '0.264*survey + 0.154*interface + 0.120*time + 0.112*computer + 0.070*user + 0.060*graph + 0.052*response + 0.049*minors + 0.045*eps + 0.036*system')\n",
      "2018-03-21 23:19:45,635 : INFO : (1, '0.255*time + 0.203*minors + 0.132*response + 0.120*trees + 0.065*computer + 0.056*eps + 0.044*system + 0.043*interface + 0.038*human + 0.029*survey')\n",
      "2018-03-21 23:19:45,636 : INFO : (2, '0.331*graph + 0.196*human + 0.103*minors + 0.102*time + 0.069*response + 0.063*system + 0.036*trees + 0.027*computer + 0.027*user + 0.018*survey')\n",
      "2018-03-21 23:19:45,637 : INFO : (3, '0.205*system + 0.187*interface + 0.121*computer + 0.112*eps + 0.097*human + 0.080*survey + 0.061*minors + 0.060*response + 0.043*user + 0.027*trees')\n",
      "2018-03-21 23:19:45,639 : INFO : (4, '0.213*graph + 0.117*eps + 0.109*minors + 0.103*trees + 0.098*computer + 0.081*survey + 0.075*response + 0.056*user + 0.053*interface + 0.048*time')\n",
      "2018-03-21 23:19:45,640 : INFO : (5, '0.259*response + 0.168*system + 0.132*time + 0.078*graph + 0.069*trees + 0.058*survey + 0.057*eps + 0.043*interface + 0.041*computer + 0.033*human')\n",
      "2018-03-21 23:19:45,641 : INFO : (6, '0.209*system + 0.186*survey + 0.159*human + 0.082*minors + 0.073*time + 0.071*response + 0.061*eps + 0.047*graph + 0.039*computer + 0.031*trees')\n",
      "2018-03-21 23:19:45,642 : INFO : (7, '0.162*interface + 0.146*time + 0.139*user + 0.127*eps + 0.123*response + 0.113*minors + 0.081*trees + 0.030*system + 0.029*computer + 0.022*human')\n",
      "2018-03-21 23:19:45,643 : INFO : (8, '0.165*system + 0.150*trees + 0.140*human + 0.137*minors + 0.134*time + 0.081*interface + 0.071*graph + 0.058*user + 0.026*response + 0.020*computer')\n",
      "2018-03-21 23:19:45,645 : INFO : (9, '0.310*survey + 0.298*computer + 0.080*minors + 0.078*graph + 0.072*user + 0.032*trees + 0.031*time + 0.031*response + 0.024*interface + 0.023*human')\n",
      "2018-03-21 23:19:45,646 : INFO : (10, '0.415*response + 0.141*time + 0.084*system + 0.076*trees + 0.074*graph + 0.071*human + 0.029*eps + 0.029*user + 0.026*interface + 0.026*computer')\n",
      "2018-03-21 23:19:45,647 : INFO : (11, '0.223*response + 0.151*interface + 0.151*user + 0.087*computer + 0.069*system + 0.064*minors + 0.056*trees + 0.054*time + 0.044*graph + 0.042*eps')\n",
      "2018-03-21 23:19:45,649 : INFO : (12, '0.421*eps + 0.152*survey + 0.118*human + 0.062*computer + 0.050*user + 0.047*response + 0.042*system + 0.038*interface + 0.035*time + 0.017*trees')\n",
      "2018-03-21 23:19:45,650 : INFO : (13, '0.194*graph + 0.155*human + 0.126*interface + 0.120*time + 0.099*eps + 0.087*trees + 0.066*user + 0.056*computer + 0.046*minors + 0.021*response')\n",
      "2018-03-21 23:19:45,651 : INFO : (14, '0.341*human + 0.194*trees + 0.161*survey + 0.065*eps + 0.063*time + 0.054*minors + 0.046*graph + 0.030*user + 0.017*response + 0.014*system')\n",
      "2018-03-21 23:19:45,652 : INFO : (15, '0.266*survey + 0.182*response + 0.153*trees + 0.100*computer + 0.072*time + 0.069*human + 0.064*graph + 0.043*eps + 0.018*minors + 0.014*interface')\n",
      "2018-03-21 23:19:45,653 : INFO : (16, '0.207*system + 0.193*trees + 0.108*survey + 0.099*graph + 0.095*human + 0.090*minors + 0.048*time + 0.043*computer + 0.041*interface + 0.033*response')\n",
      "2018-03-21 23:19:45,655 : INFO : (17, '0.402*minors + 0.204*interface + 0.158*time + 0.129*trees + 0.044*human + 0.025*response + 0.015*user + 0.010*eps + 0.007*system + 0.003*survey')\n",
      "2018-03-21 23:19:45,656 : INFO : (18, '0.236*trees + 0.166*time + 0.137*minors + 0.116*user + 0.106*system + 0.089*survey + 0.084*graph + 0.028*response + 0.017*eps + 0.016*computer')\n",
      "2018-03-21 23:19:45,657 : INFO : (19, '0.284*graph + 0.178*time + 0.150*human + 0.100*trees + 0.083*minors + 0.057*computer + 0.050*system + 0.038*eps + 0.037*survey + 0.014*interface')\n"
     ]
    }
   ],
   "source": [
    "#Gensim实现了几种流行的矢量空间模型算法：\n",
    "'''术语频率*反向文档频率，Tf-Idf 在初始化期间需要一个词袋（整数值）训练语料库。\n",
    "在转换过程中，它将采用一个向量并返回具有相同维度的另一个向量，除了训练语料库中罕见\n",
    "的特征将增加其值。因此它将整数值向量转换为实值向量，同时保持维数不变。它也可以选择\n",
    "将结果向量归一化为（欧几里得）单位长度。'''\n",
    "model = models.TfidfModel(corpus, normalize=True)\n",
    "\n",
    "'''潜在语义索引LSI（有时候称为LSA） 将文档从文字袋或TfIdf加权空间转换为低维潜在空间。\n",
    "对于上述玩具语料库，我们只使用了2个潜在维度，但在真实语料库中，目标维度为200-500\n",
    "被推荐为“黄金标准”'''\n",
    "model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)\n",
    "\n",
    "'''LSI培训的独特之处在于我们可以在任何时候继续“培训”，只需提供更多培训文档即可。这\n",
    "是通过对基础模型的增量更新完成的，这个过程称为在线培训。由于这一特性，输入文档流甚至\n",
    "可能是无限的 - 只是在LSI新文档到达时继续提供，同时将计算出的转换模型作为只读使用！'''\n",
    "# model.add_documents(another_tfidf_corpus)# now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "# lsi_vec = model[tfidf_vec] # convert some new document into the LSI space, without affecting the model\n",
    "# model.add_documents(more_documents) # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "# lsi_vec = model[tfidf_vec]\n",
    "\n",
    "#gensim使用了一种新的在线增量流式分布式训练算法\n",
    "\n",
    "'''随机投影，RP旨在减少向量空间维度。这是一种非常有效的（内存和CPU友好型）方法，通过抛\n",
    "出一点随机性来逼近文档之间的TfIdf距离。根据您的数据集，建议的目标维度也是数百/数千。'''\n",
    "model = models.RpModel(corpus_tfidf, num_topics=500)\n",
    "\n",
    "'''潜在Dirichlet分配，LDA 是从包含词计数到低维度主题空间的又一转变。LDA是LSA（也称为多项\n",
    "PCA）的概率扩展，因此LDA的主题可以解释为概率分布。这些分布与LSA一样，是从训练语料库中自动\n",
    "推断出来的。文档又被解释为这些主题的（软）混合体（再次，就像LSA一样）。'''\n",
    "model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "\n",
    "'''分层Dirichlet过程，HDP 是一种非参数贝叶斯方法（请注意缺少的请求主题数量）'''\n",
    "model = models.HdpModel(corpus, id2word=dictionary)\n",
    "\n",
    "#值得重申的是，这些都是独一无二的，增量式的实现，它们不需要一次将所有训练语料库存在主内存中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03相似性查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似的接口（Similarity interface）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面有关语料库和矢量空间以及主题和变换的教程中，我们介绍了如何在矢量空间模型中创建语料库以及如何在不同的矢量空间之间对其进行变换。这种讨论的一个常见原因是我们想要确定 文档对之间的相似性，或者特定文档与其他文档集（如用户查询与索引文档）之间的相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-22 22:00:43,886 : INFO : loading Dictionary object from deerwester.dict\n",
      "2018-03-22 22:00:43,910 : INFO : loaded deerwester.dict\n",
      "2018-03-22 22:00:43,913 : INFO : loaded corpus index from deerwester.mm.index\n",
      "2018-03-22 22:00:43,914 : INFO : initializing cython corpus reader from deerwester.mm\n",
      "2018-03-22 22:00:43,916 : INFO : accepted corpus with 9 documents, 12 features, 28 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(9 documents, 12 features, 28 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary.load('deerwester.dict')\n",
    "corpus = corpora.MmCorpus('deerwester.mm')# comes from the first tutorial, \"From strings to vectors\"\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-22 22:01:23,293 : INFO : using serial LSI version on this node\n",
      "2018-03-22 22:01:23,404 : INFO : updating model with new documents\n",
      "2018-03-22 22:01:23,748 : INFO : preparing a new chunk of documents\n",
      "2018-03-22 22:01:24,078 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-03-22 22:01:24,088 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2018-03-22 22:01:24,294 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2018-03-22 22:01:24,915 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2018-03-22 22:01:25,226 : INFO : computing the final decomposition\n",
      "2018-03-22 22:01:25,278 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2018-03-22 22:01:25,380 : INFO : processed documents up to #9\n",
      "2018-03-22 22:01:25,412 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"time\" + 0.265*\"response\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2018-03-22 22:01:25,414 : INFO : topic #1(2.542): -0.623*\"graph\" + -0.490*\"trees\" + -0.451*\"minors\" + -0.274*\"survey\" + 0.167*\"system\" + 0.141*\"eps\" + 0.113*\"human\" + -0.107*\"time\" + -0.107*\"response\" + 0.072*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "#按照Deerwester的例子，我们首先使用这个小型语料库来定义一个2维LSI空间：\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4618210045327158), (1, 0.07002766527899963)]\n"
     ]
    }
   ],
   "source": [
    "'''现在假设用户在查询“人机交互”中键入。我们希望按照与此查询相关的降序对我们的九个语料\n",
    "库文档进行排序。与现代搜索引擎不同，这里我们只关注可能相似点的一个方面 - 关于他们文本\n",
    "（词）的明显的语义相关性。没有超链接，没有随机游走静态等级，只是布尔关键字匹配的语义扩\n",
    "展：'''\n",
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]# convert the query to LSI space\n",
    "print(vec_lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-22 22:05:55,002 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2018-03-22 22:05:55,225 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x10266828>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''另外，我们将考虑余弦相似性 来确定两个向量的相似性。余弦相似性是向量空间建模中的\n",
    "一种标准度量，但向量表示概率分布的任何地方， 不同的相似性度量 可能更合适。'''\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])# transform corpus to LSI space and index it\n",
    "index\n",
    "\n",
    "'''similarities.MatrixSimilarity只有当整组矢量适合内存时，该类才适用。例如，当使用这个类时，\n",
    "一百万份文档的语料库需要在256维LSI空间中使用2GB的RAM。如果没有2GB的可用RAM，则需要使用\n",
    "similarities.Similarity该类。此类在固定内存中运行，方法是将索引拆分到磁盘上的多个文件中，\n",
    "称为碎片。它采用similarities.MatrixSimilarity和similarities.SparseMatrixSimilarity内部，\n",
    "所以还是快，虽然稍微复杂一些。'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-22 22:08:45,420 : INFO : saving MatrixSimilarity object under deerwester.index, separately None\n",
      "2018-03-22 22:08:45,634 : INFO : saved deerwester.index\n",
      "2018-03-22 22:08:45,636 : INFO : loading MatrixSimilarity object from deerwester.index\n",
      "2018-03-22 22:08:45,638 : INFO : loaded deerwester.index\n"
     ]
    }
   ],
   "source": [
    "#索引持久性通过标准save()和load()函数处理：\n",
    "index.save('deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('deerwester.index')\n",
    "\n",
    "'''这是所有类似的索引类真（similarities.Similarity， similarities.MatrixSimilarity\n",
    "和similarities.SparseMatrixSimilarity）。同样在下面，索引可以是任何这些的一个对象。\n",
    "如果有疑问，请使用similarities.Similarity它，因为它是最具可扩展性的版本，并且它还\n",
    "支持稍后向索引添加更多文档。'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]\n"
     ]
    }
   ],
   "source": [
    "#为了获得我们的查询文档与九个索引文档的相似性：\n",
    "sims = index[vec_lsi]# perform a similarity query against the corpus\n",
    "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "#余弦度量在<-1,1>范围内返回相似性（越大越相似），因此第一个文档的得分为0.99809301等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.9984453), (0, 0.998093), (3, 0.9865886), (1, 0.93748635), (4, 0.90755945), (8, 0.050041765), (7, -0.09879464), (6, -0.10639259), (5, -0.12416792)]\n"
     ]
    }
   ],
   "source": [
    "#使用一些标准的Python魔法，我们按照降序对这些相似性进行排序，并获得查询“人机交互”的最终答案：\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims) # print sorted (document number, similarity score) 2-tuples\n",
    "\n",
    "# [(2, 0.99844527), # The EPS user interface management system\n",
    "# (0, 0.99809301), # Human machine interface for lab abc computer applications\n",
    "# (3, 0.9865886), # System and human system engineering testing of EPS\n",
    "# (1, 0.93748635), # A survey of user opinion of computer system response time\n",
    "# (4, 0.90755945), # Relation of user perceived response time to error measurement\n",
    "# (8, 0.050041795), # Graph minors A survey\n",
    "# (7, -0.098794639), # Graph minors IV Widths of trees and well quasi ordering\n",
    "# (6, -0.1063926), # The intersection graph of paths in trees\n",
    "# (5, -0.12416792)] # The generation of random binary unordered trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 英语维基百科上的实验(Experiments on the English Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，从http://download.wikimedia.org/enwiki/  下载所有维基百科文章的转储 （您需要文件enwiki-latest-pages-articles.xml.bz2或enwiki-YYYYMMDD-pages-articles.xml。 bz2用于日期特定的转储）。该文件大小约为8GB，包含英文维基百科全部文章（的压缩版本）。\n",
    "\n",
    "将文章转换为纯文本（处理Wiki标记）并将结果存储为稀疏TF-IDF向量。在Python中，这很容易即时执行，我们甚至不需要将整个存档解压缩到磁盘。gensim中包含一个脚本 就是这样做的，运行：\n",
    "\n",
    "$ python -m gensim.scripts.make_wiki\n",
    "\n",
    "这个预处理步骤在8.2GB压缩维基转储（一个提取词典，一个创建和存储稀疏向量）上进行两次传递，并且在我的笔记本上花费大约9个小时，因此您可能想要喝杯咖啡或二。\n",
    "\n",
    "另外，您需要大约35GB的可用磁盘空间来存储稀疏输出向量。我建议立即压缩这些文件，例如用bzip2（低至〜13GB）。Gensim可以直接处理压缩文件，因此可以节省磁盘空间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 潜在语义分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据量太大，弄不了了\n",
    "#首先让我们加载在上面第二步中创建的语料库迭代器和词典\n",
    "import logging, gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## load id->word mapping (the dictionary), one of the results of step 2 above\n",
    "id2word = gensim.corpora.Dictionary.load_from_text(wiki_en_wordids.txt)\n",
    "# load corpus iterator\n",
    "mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n",
    "# mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output (recommended)\n",
    "print(mm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#现在我们准备计算英语维基百科的LSA：\n",
    "## extract 400 LSI topics; use the default one-pass algorithm\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=mm, id2word=id2word, num_topics=400)\n",
    "# print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "lsi.print_topics(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''gensim中使用的算法只需要查看每个输入文档一次，因此它适用于文档以不可重复流的形式出现\n",
    "的环境，或适用于多次存储/迭代整个语料库的成本过高的环境。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#潜在Dirichlet分配\n",
    "import logging, gensim\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# load id->word mapping (the dictionary), one of the results of step 2 above\n",
    "id2word = gensim.corpora.Dictionary.load_from_text('wiki_en_wordids.txt')\n",
    "# load corpus iterator\n",
    "mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm')\n",
    "# mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output\n",
    "print(mm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''我们将运行在线LDA（参见Hoffman等人[3]），这是一种接受大量文档，更新LDA模型，接受另一\n",
    "个大块，更新模型等的算法。在线LDA可以与处理整个语料库（一次全通）的批处理LDA形成对照，\n",
    "然后更新模型，然后是另一个通过，另一个更新......不同之处在于，给定一个合理固定的文档流\n",
    "（没有太多的主题漂移），小块（subcorpora）上的在线更新本身非常好，因此模型估计收敛得更快\n",
    "。因此，我们可能只需要对整个语料库进行一次完整的传递：如果语料库有300万篇文章，并且在每\n",
    "10,000篇文章之后更新一次，这意味着我们将一次完成300次更新，很可能足以有一个非常准确的话\n",
    "题估计：'''\n",
    "## extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1)\n",
    "# print the most contributing words for 20 randomly selected topics\n",
    "lda.print_topics(20)\n",
    "\n",
    "'''注意LDA和LSA运行的两个区别：我们要求LSA提取400个主题，LDA只有100个主题（所以速度差异\n",
    "实际上甚至更大）。其次，gensim中的LSA实现是真正在线的：如果输入流的性质随时间变化，LSA将\n",
    "重新定位自己以反映这些变化，并进行相当少量的更新。相比之下，LDA并不是真正的在线（ 尽管文\n",
    "献[3]的名称），因为后期更新对模型的影响逐渐减弱。如果输入文档流中存在主题漂移，则LDA会感\n",
    "到困惑，并且越来越慢地适应新的事态。\n",
    "\n",
    "简而言之，如果使用LDA随着时间的推移逐渐向模型添加新文档，请小心。LDA的批量使用，其中整个\n",
    "训练语料库事先已知或未显示主题漂移，可以，并且不受影响。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#要运行批次LDA（不在线），请使用以下方式培训LdaModel：\n",
    "# extract 100 LDA topics, using 20 full passes, no online updates\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=0, passes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#像往常一样，一个训练有素的模型可以用来将新的，看不见的文档（简单的词袋计数向量）转换为LDA主题分布\n",
    "doc_lda = lda[doc_bow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-25 10:04:06,620 : INFO : collecting all words and their counts\n",
      "2018-03-25 10:04:06,622 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-25 10:04:06,623 : INFO : collected 5 word types from a corpus of 6 raw words and 2 sentences\n",
      "2018-03-25 10:04:06,626 : INFO : Loading a fresh vocabulary\n",
      "2018-03-25 10:04:06,627 : INFO : min_count=1 retains 5 unique words (100% of original 5, drops 0)\n",
      "2018-03-25 10:04:06,628 : INFO : min_count=1 leaves 6 word corpus (100% of original 6, drops 0)\n",
      "2018-03-25 10:04:06,629 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2018-03-25 10:04:06,630 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2018-03-25 10:04:06,631 : INFO : downsampling leaves estimated 0 word corpus (7.5% of prior 6)\n",
      "2018-03-25 10:04:06,632 : INFO : estimated required memory for 5 words and 100 dimensions: 6500 bytes\n",
      "2018-03-25 10:04:06,633 : INFO : resetting layer weights\n",
      "2018-03-25 10:04:06,646 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-25 10:04:06,705 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:04:06,706 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:04:06,707 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:04:06,709 : INFO : EPOCH - 1 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:04:06,712 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:04:06,713 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:04:06,714 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:04:06,715 : INFO : EPOCH - 2 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:04:06,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:04:06,721 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:04:06,723 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:04:06,725 : INFO : EPOCH - 3 : training on 6 raw words (1 effective words) took 0.0s, 150 effective words/s\n",
      "2018-03-25 10:04:06,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:04:06,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:04:06,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:04:06,731 : INFO : EPOCH - 4 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:04:06,735 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:04:06,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:04:06,739 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:04:06,740 : INFO : EPOCH - 5 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:04:06,742 : INFO : training on a 30 raw words (1 effective words) took 0.1s, 11 effective words/s\n",
      "2018-03-25 10:04:06,743 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.6672794e-03,  2.3785748e-03, -1.2979079e-03, -2.2986489e-03,\n",
       "        3.8589868e-03,  4.7677383e-03, -8.4622815e-04, -4.7469721e-03,\n",
       "        4.5546829e-03, -2.1930039e-03,  7.8014639e-04, -1.7201888e-03,\n",
       "       -2.5572879e-03, -3.7275692e-03, -1.2652479e-03, -2.4822517e-03,\n",
       "       -3.3013052e-03, -3.9827703e-03,  7.9437724e-04, -2.5375409e-03,\n",
       "        3.2035625e-03,  4.6119727e-03, -4.8709619e-03, -4.3224655e-03,\n",
       "        3.8996288e-03, -7.2479033e-04, -1.1487225e-03,  3.0452790e-04,\n",
       "       -3.0824414e-03, -1.0298850e-03,  3.1206936e-03,  4.0110103e-03,\n",
       "       -2.7508582e-03, -3.9044614e-03, -2.9462778e-03, -2.9102932e-03,\n",
       "       -2.8171660e-03,  2.9010044e-03, -2.3535555e-04,  5.0747598e-04,\n",
       "        2.3167240e-03,  2.7977065e-03, -4.0146853e-03,  1.2916119e-03,\n",
       "        1.6181866e-03,  2.6329991e-03,  7.3889247e-04,  2.7017347e-03,\n",
       "       -2.2440269e-03, -8.5438509e-04, -1.2963975e-03,  3.9401818e-03,\n",
       "        8.8969828e-04,  2.4732088e-03,  3.1242839e-03,  3.3276102e-03,\n",
       "        8.4508961e-04, -2.4276536e-03,  4.3029645e-03, -1.3404881e-03,\n",
       "        4.0744916e-03,  2.9513920e-03, -3.2776007e-03,  2.3443066e-03,\n",
       "        4.5965882e-03,  1.0962053e-03,  2.9436380e-03, -4.4767288e-03,\n",
       "       -4.4379318e-03, -3.4044278e-03, -3.1307579e-03, -1.9044373e-03,\n",
       "        1.7251865e-03,  3.2763027e-03,  4.4149443e-04,  4.8378883e-03,\n",
       "       -3.6865682e-03, -2.6173042e-03,  1.7665826e-03, -1.7436339e-03,\n",
       "        4.7419644e-03, -2.7618576e-03,  3.8344136e-03,  4.7529880e-03,\n",
       "       -1.5046590e-03, -2.2366408e-03, -3.7100934e-03, -1.7649428e-03,\n",
       "        5.1814681e-05,  4.5123026e-03, -4.1224231e-04,  7.8948081e-04,\n",
       "        2.3747131e-03,  3.0942759e-03,  2.2247841e-03, -2.3303297e-03,\n",
       "       -3.8113645e-03, -3.8254734e-03,  2.6939623e-03,  4.2399559e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "say_vector = model['say']  # get vector for word\n",
    "say_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "say_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [model[word] for word in sentences]\n",
    "len(temp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-25 10:19:46,263 : INFO : collecting all words and their counts\n",
      "2018-03-25 10:19:46,264 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-25 10:19:46,268 : INFO : collected 5 word types from a corpus of 6 raw words and 2 sentences\n",
      "2018-03-25 10:19:46,271 : INFO : Loading a fresh vocabulary\n",
      "2018-03-25 10:19:46,273 : INFO : min_count=1 retains 5 unique words (100% of original 5, drops 0)\n",
      "2018-03-25 10:19:46,274 : INFO : min_count=1 leaves 6 word corpus (100% of original 6, drops 0)\n",
      "2018-03-25 10:19:46,276 : INFO : deleting the raw counts dictionary of 5 items\n",
      "2018-03-25 10:19:46,278 : INFO : sample=0.001 downsamples 5 most-common words\n",
      "2018-03-25 10:19:46,279 : INFO : downsampling leaves estimated 0 word corpus (7.5% of prior 6)\n",
      "2018-03-25 10:19:46,359 : INFO : estimated required memory for 5 words, 38 buckets and 100 dimensions: 22244 bytes\n",
      "2018-03-25 10:19:46,400 : INFO : resetting layer weights\n",
      "2018-03-25 10:19:46,829 : INFO : Total number of ngrams is 38\n",
      "2018-03-25 10:19:46,996 : INFO : training model with 3 workers on 5 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-03-25 10:19:47,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:19:47,026 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:19:47,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:19:47,038 : INFO : EPOCH - 1 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:19:47,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:19:47,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:19:47,051 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:19:47,054 : INFO : EPOCH - 2 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:19:47,057 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:19:47,058 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:19:47,060 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:19:47,062 : INFO : EPOCH - 3 : training on 6 raw words (1 effective words) took 0.0s, 214 effective words/s\n",
      "2018-03-25 10:19:47,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:19:47,070 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:19:47,071 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:19:47,072 : INFO : EPOCH - 4 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:19:47,075 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-25 10:19:47,078 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-25 10:19:47,080 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-25 10:19:47,081 : INFO : EPOCH - 5 : training on 6 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2018-03-25 10:19:47,083 : INFO : training on a 30 raw words (1 effective words) took 0.1s, 12 effective words/s\n",
      "2018-03-25 10:19:47,085 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.48374348e-03  1.28183200e-03  2.78071064e-04  1.17748801e-03\n",
      " -2.84763589e-03  9.77993477e-04  7.77854701e-04  3.20158107e-03\n",
      " -3.23760835e-03  7.47663376e-04 -2.53082108e-04 -1.91538595e-03\n",
      "  4.54709720e-04 -5.62755333e-04 -3.87221808e-03  1.28175726e-03\n",
      " -1.76023715e-03  7.72499421e-04  1.95931108e-03 -2.86182249e-03\n",
      " -4.55376838e-04  3.05658067e-03  2.28731777e-03  8.24262388e-05\n",
      " -1.02034304e-03  3.90086439e-03 -2.50886264e-03 -1.91701506e-03\n",
      " -2.79970840e-03 -6.84790313e-04 -3.51232733e-03  7.81246563e-05\n",
      " -1.41762511e-03  3.18209408e-03  1.07611215e-03 -3.61893000e-03\n",
      "  1.30862638e-03  1.16036169e-03 -3.37773818e-03  4.24797880e-03\n",
      " -1.71071826e-03  6.81362930e-04 -2.27297656e-03  2.86741694e-03\n",
      " -1.21385325e-04  5.62858162e-03  2.79778475e-03  1.07019977e-03\n",
      " -1.81176583e-03 -1.60234573e-03 -2.64236447e-03  2.93838070e-03\n",
      " -1.13120431e-03 -1.86256308e-03  7.85981014e-04 -1.54724543e-03\n",
      "  1.97424321e-03 -3.16070416e-03  2.61859177e-03 -2.08307451e-04\n",
      " -1.47058594e-03  2.41366611e-03 -2.48487631e-04 -2.81691144e-04\n",
      " -1.76252646e-03  7.45113415e-04  6.66863751e-04  1.58800802e-03\n",
      "  1.14193745e-03  1.15941395e-03  1.54933857e-03 -1.27576047e-03\n",
      " -6.34108577e-03 -1.37676683e-03 -1.15316280e-03  1.80655182e-03\n",
      " -1.02765451e-03  1.99828832e-03  2.50652171e-04 -2.68008414e-04\n",
      "  3.75274045e-04 -1.87061226e-03 -1.40709907e-03 -1.01007638e-03\n",
      "  2.29995325e-03 -1.76212727e-03  1.02789409e-03  2.73411768e-03\n",
      " -4.81660885e-04  4.31985082e-03  2.37344019e-03  1.92553585e-03\n",
      " -7.25806982e-04 -5.07429882e-04  1.36231747e-03  2.96097668e-03\n",
      "  5.31877065e-03  2.80293432e-04  1.67569960e-03  5.12147695e-03]\n",
      "[-9.8086623e-03 -4.9609714e-03  1.1352926e-04  8.0833299e-04\n",
      " -2.5741516e-03 -4.1457727e-03 -5.9526530e-03  6.0056378e-03\n",
      " -3.6815419e-03 -1.5064029e-04  6.2258178e-03  2.2471789e-03\n",
      " -1.8278301e-03 -6.4619156e-03 -2.0469583e-03 -2.9574693e-04\n",
      "  9.8116919e-03 -4.3160985e-03 -6.6872067e-03 -5.5884896e-03\n",
      " -5.7706530e-03 -8.4790466e-03 -2.7018460e-03 -4.3053194e-03\n",
      "  7.7517759e-03  5.6051924e-03 -7.8187007e-03  7.0100697e-03\n",
      " -7.8080278e-03  4.4041551e-03  1.4564859e-03 -8.2638320e-03\n",
      "  8.1065604e-03  7.4913302e-03  6.8770344e-03 -2.6086529e-03\n",
      " -5.5409716e-03  9.9422336e-03  6.3467673e-03 -9.1574369e-03\n",
      " -2.5344423e-03  6.2437397e-03  5.6769396e-04 -5.0573833e-03\n",
      "  5.8577587e-03 -4.7649792e-03 -9.9920323e-03 -5.3736218e-03\n",
      "  9.8674055e-03  3.1240168e-03 -5.5866376e-03  3.7227941e-03\n",
      "  3.7387379e-03  2.8148554e-03 -8.5890293e-03 -5.1407176e-03\n",
      "  7.4167438e-03  4.6948162e-03 -6.6436799e-03 -4.0282109e-03\n",
      "  1.2544405e-03  6.5289154e-03 -5.9896740e-03 -7.9666944e-03\n",
      " -2.3626222e-03  8.2470691e-03 -4.7897296e-03 -7.8339130e-03\n",
      " -8.8811405e-03 -2.9665888e-03 -9.2474613e-03  1.6606784e-03\n",
      "  3.4511362e-03 -7.9032819e-04  4.8223371e-03  2.0499288e-05\n",
      "  4.0470525e-03 -5.3871819e-03  4.8922482e-03  3.2744039e-05\n",
      "  4.1073766e-03 -1.0660442e-03  1.1881219e-04 -9.1692964e-03\n",
      " -2.1381419e-04 -7.1542501e-03 -7.1500828e-03 -1.3329920e-03\n",
      "  1.5242958e-03 -3.1390260e-03 -6.7901681e-03  4.8282081e-03\n",
      "  9.2618316e-03  4.6728458e-03 -8.4587149e-03  6.8181632e-03\n",
      " -8.6497189e-03 -1.2737591e-03  8.5772555e-03 -4.7363979e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "model = FastText(sentences, min_count=1)\n",
    "say_vector = model['say']  # get vector for word\n",
    "of_vector = model['of']  # get vector for out-of-vocab word\n",
    "print(say_vector)\n",
    "print(of_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
